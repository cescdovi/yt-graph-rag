{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5885d041",
   "metadata": {},
   "source": [
    "# CONSTRUIR BD EN NEO4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9244e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "class Persona(BaseModel):\n",
    "    id: str = Field(default_factory=lambda: str(uuid4()))\n",
    "    nombre: str\n",
    "    edad: int\n",
    "    ciudad: str\n",
    "\n",
    "\n",
    "class Coche(BaseModel):\n",
    "    id: str = Field(default_factory=lambda: str(uuid4()))\n",
    "    marca: str\n",
    "    modelo: str\n",
    "    anio: int\n",
    "\n",
    "class Entidades(BaseModel):\n",
    "    \"\"\"Contenedor de todas las entidades\"\"\"\n",
    "    personas: List[Persona] = Field(default_factory=list)\n",
    "    coches: List[Coche] = Field(default_factory=list)\n",
    "\n",
    "#relaciones\n",
    "class Viaje(BaseModel):\n",
    "    id: str = Field(default_factory=lambda: str(uuid4()))\n",
    "    persona: Persona\n",
    "    coche: Coche\n",
    "    descripcion_relacion: str\n",
    "\n",
    "class Relaciones(BaseModel):\n",
    "    \"\"\"Contenedor de todos los pares relacionados\"\"\"\n",
    "    relaciones: List[Viaje] = Field(default_factory=list)\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    entidades: Entidades\n",
    "    relaciones: Relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcb87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personas\n",
    "juan = Persona(nombre=\"Juan\", edad=30, ciudad=\"Madrid\")\n",
    "ana = Persona(nombre=\"Ana\", edad=25, ciudad=\"Barcelona\")\n",
    "luis = Persona(nombre=\"Luis\", edad=40, ciudad=\"Valencia\")\n",
    "\n",
    "# Coches\n",
    "toyota = Coche(marca=\"Toyota\", modelo=\"Corolla\", anio=2018)\n",
    "ford   = Coche(marca=\"Ford\", modelo=\"Focus\", anio=2020)\n",
    "tesla  = Coche(marca=\"Tesla\", modelo=\"Model 3\", anio=2022)\n",
    "\n",
    "# Viajes\n",
    "viaje1 = Viaje(persona=juan, coche=toyota,descripcion_relacion=\"Fue su primer coche\")\n",
    "viaje2 = Viaje(persona=ana, coche=tesla,  descripcion_relacion=\"Viajó con su madre para ver el nuevo modelo\")\n",
    "viaje3 = Viaje(persona=luis, coche=ford,  descripcion_relacion=\"Tuvo un accidente y no pudo viajar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592a7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = Entidades(personas=[juan, ana, luis], coches=[toyota, ford, tesla])\n",
    "relaciones = Relaciones(relaciones=[viaje1, viaje2, viaje3])\n",
    "output_schema = OutputSchema(entidades=entidades, relaciones=relaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conectar a Neo4j\n",
    "graph = Neo4jGraph(url=\"bolt://localhost:50001\", username=\"neo4j\", password=\"frandovi\", database = \"prueba-db\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd05ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Persona {id: STRING, edad: INTEGER, ciudad: STRING, nombre: STRING}\n",
      "Coche {id: STRING, anio: INTEGER, marca: STRING, modelo: STRING}\n",
      "Relationship properties:\n",
      "VIAJO_EN {descripcion_relacion: STRING}\n",
      "The relationships:\n",
      "(:Persona)-[:VIAJO_EN]->(:Coche)\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7814b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar todos los nodos y relaciones\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# eliminar constraints/índices si los creaste\n",
    "graph.query(\"DROP CONSTRAINT persona_id IF EXISTS\")\n",
    "graph.query(\"DROP CONSTRAINT coche_id IF EXISTS\")\n",
    "graph.query(\"DROP CONSTRAINT viaje_rel_id IF EXISTS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abbea30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Persona(id='bdb38422-17cd-43df-a910-f041624aa134', nombre='Juan', edad=30, ciudad='Madrid'),\n",
       " Persona(id='e3bdcf26-a0b5-4af0-8c2f-402dc1b24271', nombre='Ana', edad=25, ciudad='Barcelona'),\n",
       " Persona(id='f9643973-b84b-4c49-8705-92acd488047c', nombre='Luis', edad=40, ciudad='Valencia')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_schema.entidades.personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f77147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# constraints\n",
    "graph.query(\"\"\"\n",
    "CREATE CONSTRAINT persona_id IF NOT EXISTS\n",
    "FOR (p:Persona) REQUIRE p.id IS UNIQUE;\n",
    "\"\"\")\n",
    "graph.query(\"\"\"\n",
    "CREATE CONSTRAINT coche_id IF NOT EXISTS\n",
    "FOR (c:Coche) REQUIRE c.id IS UNIQUE;\n",
    "\"\"\")\n",
    "graph.query(\"\"\"\n",
    "CREATE CONSTRAINT viaje_rel_id IF NOT EXISTS\n",
    "FOR ()-[v:VIAJO_EN]-() REQUIRE v.id IS UNIQUE;\n",
    "\"\"\")\n",
    "\n",
    "#ENTIDADES\n",
    "##personas\n",
    "for persona in output_schema.entidades.personas:\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE (p:Persona {id: $id})\n",
    "        SET p.nombre = $nombre, p.edad = $edad, p.ciudad = $ciudad\n",
    "        \"\"\",\n",
    "        params = {\n",
    "            \"id\": persona.id,\n",
    "            \"nombre\": persona.nombre,\n",
    "            \"edad\": persona.edad,\n",
    "            \"ciudad\": persona.ciudad\n",
    "        }\n",
    "\n",
    "    )\n",
    "\n",
    "##coches\n",
    "for coche in output_schema.entidades.coches:\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE (c:Coche {id: $id})\n",
    "        SET c.marca = $marca, c.modelo = $modelo, c.anio = $anio\n",
    "        \"\"\",\n",
    "        params = {\n",
    "            \"id\": coche.id,\n",
    "            \"marca\": coche.marca,\n",
    "            \"modelo\": coche.modelo,\n",
    "            \"anio\": coche.anio\n",
    "        }\n",
    "    )\n",
    "\n",
    "for viaje in output_schema.relaciones.relaciones:\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (p:Persona {id: $persona_id}), (c:Coche {id: $coche_id})\n",
    "        MERGE (p)-[r:VIAJO_EN {descripcion_relacion: $descripcion_relacion}]->(c)\n",
    "        \"\"\",\n",
    "        params={\n",
    "            \"persona_id\": viaje.persona.id,\n",
    "            \"coche_id\": viaje.coche.id,\n",
    "            \"descripcion_relacion\": viaje.descripcion_relacion\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cc596",
   "metadata": {},
   "source": [
    "# CONSULTAS A LA BD EN LENGUAJE NATURAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f1d74",
   "metadata": {},
   "source": [
    "## CONVERTIR LENGUAJE NATURAL A CYPHER\n",
    "- Cadena que convierte una consulta de lenguaje natural a lenguaje cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b93e6e",
   "metadata": {},
   "source": [
    "- Para administrar permisos, se necesita versión Enterprise, la gratuita es community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae3d3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# graph.query(\"\"\"\n",
    "# CREATE USER lector SET PASSWORD 'tu_password' CHANGE NOT REQUIRED;\n",
    "# GRANT MATCH {*} ON GRAPH * TO lector;\n",
    "# DENY WRITE {*} ON GRAPH * TO lector;\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07a7f1",
   "metadata": {},
   "source": [
    "- Esto es lo que ocurre internamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78085fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], \n",
    "    template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e04eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Persona {nombre: STRING, edad: INTEGER, ciudad: STRING, id: STRING}\n",
      "Coche {modelo: STRING, anio: INTEGER, marca: STRING, id: STRING}\n",
      "Relationship properties:\n",
      "VIAJO_EN {descripcion_relacion: STRING}\n",
      "The relationships:\n",
      "(:Persona)-[:VIAJO_EN]->(:Coche)\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d088932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Persona {id: STRING, edad: INTEGER, ciudad: STRING, nombre: STRING}\n",
      "Coche {id: STRING, anio: INTEGER, marca: STRING, modelo: STRING}\n",
      "Relationship properties:\n",
      "VIAJO_EN {descripcion_relacion: STRING}\n",
      "The relationships:\n",
      "(:Persona)-[:VIAJO_EN]->(:Coche)\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecaa68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "Node properties:\n",
      "Persona {id: STRING, edad: INTEGER, ciudad: STRING, nombre: STRING}\n",
      "Coche {id: STRING, anio: INTEGER, marca: STRING, modelo: STRING}\n",
      "Relationship properties:\n",
      "VIAJO_EN {descripcion_relacion: STRING}\n",
      "The relationships:\n",
      "(:Persona)-[:VIAJO_EN]->(:Coche)\n",
      "\n",
      "The question is:\n",
      "Quién viajó en un tesla?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_text = CYPHER_GENERATION_PROMPT.format(\n",
    "    schema=graph.schema, \n",
    "    question=\"Quién viajó en un tesla?\"\n",
    ")\n",
    "\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfd10ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Tarea: Genera una sentencia Cypher para consultar una base de datos de grafos.\n",
    "Instrucciones:\n",
    "- Usa únicamente los tipos de nodos, relaciones y propiedades que aparecen en el esquema.\n",
    "- NO inventes etiquetas, relaciones o propiedades.\n",
    "- **En el RETURN usa SIEMPRE alias fijos: p.nombre AS nombre, c.marca AS marca, c.modelo AS modelo, c.anio AS anio.**\n",
    "\n",
    "Esquema:\n",
    "{schema}\n",
    "\n",
    "Nota: \n",
    "- No incluyas explicaciones ni disculpas.\n",
    "- Devuelve únicamente la sentencia Cypher generada.\n",
    "\n",
    "Ejemplos:\n",
    "# ¿En qué coches viajó Juan?\n",
    "MATCH (p:Persona {{nombre:\"Juan\"}})-[:VIAJO_EN]->(c:Coche)\n",
    "RETURN c.marca AS marca, c.modelo AS modelo\n",
    "\n",
    "# ¿Cuántas personas viajaron en un coche de marca Toyota?\n",
    "MATCH (p:Persona)-[:VIAJO_EN]->(c:Coche {{marca:\"Toyota\"}})\n",
    "RETURN count(p) AS numeroDePersonas\n",
    "\n",
    "La pregunta es:\n",
    "{question}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\",\"question\"],\n",
    "    template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Eres un asistente que responde EXCLUSIVAMENTE con el contexto (lista de diccionarios en JSON).\\n\"\n",
    "        \"Reglas:\\n\"\n",
    "        \"1) Si el contexto está vacío (lista vacía), responde EXACTAMENTE: 'No se encontraron resultados'.\\n\"\n",
    "        \"2) Si el contexto NO está vacío, NUNCA respondas 'No se encontraron resultados'. Debes responder con los datos.\\n\"\n",
    "        \"3) Si alguna fila tiene una clave 'nombre' o 'nombrePersona', devuelve los valores únicos de esas claves, separados por comas y sin texto adicional.\\n\"\n",
    "        \"4) Si la pregunta pide un conteo y hay una clave 'numeroDePersonas' o 'count', devuelve solo el número.\\n\"\n",
    "        \"5) Si no aplican las reglas anteriores, devuelve un resumen mínimo usando los campos disponibles del contexto.\\n\\n\"\n",
    "        \"Contexto:\\n{context}\\n\\n\"\n",
    "        \"Pregunta:\\n{question}\\n\\n\"\n",
    "        \"Respuesta breve:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    graph=graph,                            # usa el schema directamente del grafo\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT, # usa info del grafo como contexto (nodos, relaciones)\n",
    "    qa_prompt=QA_PROMPT,                    # instrucciones para generar la respuesta\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "051a736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Persona {nombre:\"Ana\"})-[:VIAJO_EN]->(c:Coche {marca:\"Tesla\"})\n",
      "RETURN p.nombre AS nombre, c.marca AS marca, c.modelo AS modelo, c.anio AS anio\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'nombre': 'Ana', 'marca': 'Tesla', 'modelo': 'Model 3', 'anio': 2022}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'Ana tuvo un tesla?', 'result': 'Sí.'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"query\":\"Ana tuvo un tesla?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc618aba",
   "metadata": {},
   "source": [
    "## USAR EMBEDDINGS\n",
    "- Se almacena un embedding usando ciertos campos de cada nodo\n",
    "\n",
    "Pasos:\n",
    "1. Construir un texto representativo de cada nodo a indexar que contiene un texto plano con el contenido de todas las propiedades de dicho nodo nodo.\n",
    "\n",
    "    - EJEMPLO PERSONA:\n",
    "        ```python\n",
    "        \"Persona: Juan, edad: 30, ciudad: Madrid\"\n",
    "        ```\n",
    "    ----\n",
    "    \n",
    "    - PROBLEMA: En Neo4j los índices vectoriales solo aplican a nodos, así que hay que “materializar” cada relación como un nodo `:Doc` o `:Viaje`\n",
    "        - EJEMPLO:\n",
    "            ```python\n",
    "            \"Viaje: Luis con coche Ford Focus (2020). Detalle: Tuvo un accidente y no pudo viajar\"\n",
    "            ```\n",
    "\n",
    "\n",
    "2. Guardar ese texto en una propiedad (`text`) del nodo \n",
    "3. Calcular y guardar el embedding en otra propiedad (`embedding`).\n",
    "4. Crear el índice vectorial en Neo4j.\n",
    "5. Usar Neo4jVector (LangChain) para recuperar por similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb10470",
   "metadata": {},
   "source": [
    "Se pueden pasar varias propiedades en lugar de generar una nueva con todos los campos, pero se pierde coherencia, vería así: \n",
    "```python\n",
    "- \"Juan Madrid 30\"\n",
    "- \"Ana Barcelona 25\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4ec2f",
   "metadata": {},
   "source": [
    "1. Construir un texto representativo de cada nodo a indexar que contiene un texto plano con el contenido de todas las propiedades de dicho nodo nodo.\n",
    "2. Guardar ese texto en una propiedad (`text`) del nodo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0daa2f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'c.marca': 'Toyota',\n",
       "  'c.modelo': 'Corolla',\n",
       "  'c.text': 'Coche: Toyota Corolla, año: 2018'},\n",
       " {'c.marca': 'Ford',\n",
       "  'c.modelo': 'Focus',\n",
       "  'c.text': 'Coche: Ford Focus, año: 2020'},\n",
       " {'c.marca': 'Tesla',\n",
       "  'c.modelo': 'Model 3',\n",
       "  'c.text': 'Coche: Tesla Model 3, año: 2022'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# personas\n",
    "graph.query(\"\"\"\n",
    "MATCH (p:Persona)\n",
    "SET p.text = 'Persona: ' + p.nombre +\n",
    "             ', edad: ' + toString(p.edad) +\n",
    "             ', ciudad: ' + coalesce(p.ciudad, '')\n",
    "RETURN p.nombre, p.text;\n",
    "\"\"\")\n",
    "\n",
    "# coches\n",
    "graph.query(\"\"\"\n",
    "MATCH (c:Coche)\n",
    "SET c.text = 'Coche: ' + c.marca +\n",
    "             ' ' + c.modelo +\n",
    "             ', año: ' + toString(c.anio)\n",
    "RETURN c.marca, c.modelo, c.text;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d25ed0",
   "metadata": {},
   "source": [
    "- Tener en cuenta que para las relaciones hay que materializarlas en un nodo relacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35c47fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function: `id`.} {position: line: 3, column: 21, offset: 63} for query: \"\\nMATCH (p:Persona)-[v:VIAJO_EN]->(c:Coche)\\nMERGE (d:Viaje {id: id(v)})   // crea un nodo Viaje por cada relación\\nSET d.text = 'Viaje: ' + p.nombre +\\n             ' con coche ' + c.marca + ' ' + c.modelo + \\n             ' (' + toString(c.anio) + '). ' +\\n             'Detalle: ' + coalesce(v.descripcion_relacion, '')\\nMERGE (d)-[:DE_PERSONA]->(p)\\nMERGE (d)-[:EN_COCHE]->(c)\\nRETURN d.text;\\n\\n\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d.text': 'Viaje: Juan con coche Toyota Corolla (2018). Detalle: Fue su primer coche'},\n",
       " {'d.text': 'Viaje: Ana con coche Tesla Model 3 (2022). Detalle: Viajó con su madre para ver el nuevo modelo'},\n",
       " {'d.text': 'Viaje: Luis con coche Ford Focus (2020). Detalle: Tuvo un accidente y no pudo viajar'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\"\"\"\n",
    "MATCH (p:Persona)-[v:VIAJO_EN]->(c:Coche)\n",
    "MERGE (d:Viaje {id: id(v)})   // crea un nodo Viaje por cada relación\n",
    "SET d.text = 'Viaje: ' + p.nombre +\n",
    "             ' con coche ' + c.marca + ' ' + c.modelo + \n",
    "             ' (' + toString(c.anio) + '). ' +\n",
    "             'Detalle: ' + coalesce(v.descripcion_relacion, '')\n",
    "MERGE (d)-[:DE_PERSONA]->(p)\n",
    "MERGE (d)-[:EN_COCHE]->(c)\n",
    "RETURN d.text;\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0c28193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Persona {id: STRING, edad: INTEGER, ciudad: STRING, nombre: STRING}\n",
      "Coche {id: STRING, anio: INTEGER, marca: STRING, modelo: STRING}\n",
      "Relationship properties:\n",
      "VIAJO_EN {descripcion_relacion: STRING}\n",
      "The relationships:\n",
      "(:Persona)-[:VIAJO_EN]->(:Coche)\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750c854",
   "metadata": {},
   "source": [
    "3. Calcular y guardar el embedding en otra propiedad (`embedding`).\n",
    "4. Crear el índice vectorial en Neo4j.\n",
    "5. Usar Neo4jVector (LangChain) para recuperar por similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jVector\n",
    "\n",
    "\n",
    "EMBEDDINGS_MODEL = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Neo4jVector.from_existing_graph(\n",
    "    url=\"bolt://localhost:50001\", \n",
    "    username=\"neo4j\", \n",
    "    password=\"frandovi\", \n",
    "    database = \"prueba-db\",\n",
    "    embedding=EMBEDDINGS_MODEL, #modelo para calcular embeddings\n",
    "    node_label=\"Viaje\", #nodos sobre los que se construye el indica vectorial\n",
    "    text_node_properties=[\"text\"], #propiedad que contiene el texto sobre el que se calcula el embedding\n",
    "    embedding_node_property=\"embedding\", #propiedad en la que se almacenan los embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "662742ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7064b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Ana tiene un Tesla Model 3 (2022).\n",
      "----- Fuentes -----\n",
      "\n",
      "text: Viaje: Ana con coche Tesla Model 3 (2022). Detalle: Viajó con su madre para ver el nuevo modelo {}\n",
      "\n",
      "text: Viaje: Juan con coche Toyota Corolla (2018). Detalle: Fue su primer coche {}\n"
     ]
    }
   ],
   "source": [
    "res = qa.invoke({\"query\": \"¿Quién tiene un tesla?\"})\n",
    "print(res[\"result\"])\n",
    "print(\"----- Fuentes -----\")\n",
    "for doc in res[\"source_documents\"]:\n",
    "    print(doc.page_content, doc.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b7633",
   "metadata": {},
   "source": [
    "## Devolver nodos más relevantes de la BD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74002b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text: Viaje: Luis con coche Ford Focus (2020). Detalle: Tuvo un accidente y no pudo viajar {}\n",
      "\n",
      "text: Viaje: Juan con coche Toyota Corolla (2018). Detalle: Fue su primer coche {}\n"
     ]
    }
   ],
   "source": [
    "# solo textos/nodos (top-k)\n",
    "docs = vectorstore.similarity_search(\"accidente en un viaje\", k=2)\n",
    "for d in docs:\n",
    "    print(d.page_content, d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b9c0a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6142463684082031 \n",
      "text: Viaje: Juan con coche Toyota Corolla (2018). Detalle: Fue su primer coche {}\n"
     ]
    }
   ],
   "source": [
    "# con puntuación de similitud\n",
    "docs_scores = vectorstore.similarity_search_with_score(\"corola\", k=1)\n",
    "for d, score in docs_scores:\n",
    "    print(score, d.page_content, d.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02605900",
   "metadata": {},
   "source": [
    "# Crear agente\n",
    "Pasos\n",
    "1. Definir modelo para el tool calling\n",
    "2. Definir tolls\n",
    "3. Definir PromptTemplate(System y Human)\n",
    "4. Definir Agent\n",
    "5. Definir AgentExecutor\n",
    "6. Ejecutar AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33894dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=Path.cwd().parents[1] / \".env\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", None)\n",
    "print(TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f24e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_tavily import TavilySearch\n",
    "# 1. Definir modelo para el tool calling\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "#2. Definir tolls\n",
    "tavily_tool = TavilySearch(\n",
    "    include_answer=True,\n",
    "    include_raw_content=False,\n",
    "    max_results=5,\n",
    ")\n",
    "\n",
    "tools = [tavily_tool]\n",
    "\n",
    "# 3. Definir PromptTemplate(System y Human)\n",
    "system = (\n",
    "    \"Eres un agente útil. \"\n",
    "    \"Cuando la pregunta requiera información actual o fuentes, usa la tool de búsqueda web. \"\n",
    "    \"Cuando se pida información del grafo, razona y luego usa la tool neo4j_query con consultas READ (MATCH/RETURN). \"\n",
    "    \"Devuelve respuestas concisas y con pasos si ejecutaste tools.\"\n",
    ")\n",
    "human = \"{input}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    MessagesPlaceholder(variable_name = \"chat_history\"), #hueco en el prompt donde se inserta el historial de conversación  \n",
    "    (\"human\", human),\n",
    "    MessagesPlaceholder(variable_name = \"agent_scratchpad\"), #hueco donde el agente guarda su razonamiento intermedio (tools...)\n",
    "\n",
    "])\n",
    "\n",
    "# 4) Construir el runnable del agente\n",
    "agent_runnable = create_tool_calling_agent(\n",
    "    llm, \n",
    "    tools, \n",
    "    prompt,\n",
    "    #handle_parsing_errors=True #si no quieres que responda sin tool\n",
    "    )\n",
    "\n",
    "# 5) Ejecutable final\n",
    "agent = AgentExecutor(agent=agent_runnable, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "99c53603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x109003b00>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x109003b00>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un agente útil. Cuando la pregunta requiera información actual o fuentes, usa la tool de búsqueda web. Cuando se pida información del grafo, razona y luego usa la tool neo4j_query con consultas READ (MATCH/RETURN). Devuelve respuestas concisas y con pasos si ejecutaste tools.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c75bafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_redis import RedisChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain_core.messages import BaseMessage\n",
    "class RedisSummaryMemory:\n",
    "    def __init__(self, session_id, redis_url, llm, summary_every=4, ttl=60*60*24*7):\n",
    "\n",
    "\n",
    "        # clase que permite conectarse a una BD de redis para gestionar la memoria del chat\n",
    "        self.redis_history = RedisChatMessageHistory(session_id=session_id, redis_url=redis_url, ttl=ttl)\n",
    "        self.llm = llm\n",
    "        self.summary_every = summary_every\n",
    "    \n",
    "    @property\n",
    "    def messages(self):\n",
    "        # Retorna mensajes de forma síncrona o asíncrona según tu entorno\n",
    "        # Si RedisChatMessageHistory.messages es async, hazlo async también\n",
    "        return self.redis_history.messages\n",
    "\n",
    "\n",
    "    async def add_user_message(self, content):\n",
    "        await self.redis_history.add_user_message(content)\n",
    "        await self._maybe_summarize()\n",
    "\n",
    "    async def add_ai_message(self, content):\n",
    "        await self.redis_history.add_ai_message(content)\n",
    "        await self._maybe_summarize()\n",
    "\n",
    "    async def _maybe_summarize(self):\n",
    "        messages = self.redis_history.messages\n",
    "        if len(messages) >= self.summary_every:\n",
    "            # Construir texto para resumen\n",
    "            conversation_text = \"\\n\".join([f\"{m.type}: {m.content}\" for m in messages])\n",
    "            prompt = f\"Resume brevemente esta conversación:\\n{conversation_text}\\nResumen:\"\n",
    "            response = await self.llm.agenerate([HumanMessage(content=prompt)])\n",
    "            summary = response.generations[[0]].text\n",
    "            # Limpiar mensajes antiguos y guardar resumen como mensaje AI\n",
    "            await self.redis_history.clear()\n",
    "            await self.redis_history.add_ai_message(f\"Resumen de la conversación: {summary}\")\n",
    "    \n",
    "    async def aadd_messages(self, messages: list[BaseMessage]):\n",
    "        for message in messages:\n",
    "            if message.type == \"human\":\n",
    "                await self.add_user_message(message.content)\n",
    "            elif message.type == \"ai\":\n",
    "                await self.add_ai_message(message.content)\n",
    "            else:\n",
    "                # Maneja otros tipos si es necesario\n",
    "                pass\n",
    "\n",
    "    async def aget_messages(self):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: self.redis_history.messages)\n",
    "\n",
    "\n",
    "\n",
    "#conectar a una sesion especifica de la BD y obtener historial de mensajes,\n",
    "#se borran a los 7 dias\n",
    "# def get_history(session_id: str):\n",
    "#     return RedisSummaryMemory(\n",
    "#         session_id=session_id,\n",
    "#         redis_url=\"redis://localhost:6380/0\",\n",
    "#         llm=llm,\n",
    "#         summary_every=4,\n",
    "#         ttl=60*60*24*7\n",
    "#     )\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    return RedisChatMessageHistory(session_id=session_id, redis_url=\"redis://localhost:6380/0\", ttl=None)\n",
    "\n",
    "agent_with_memory = RunnableWithMessageHistory(\n",
    "    runnable = agent, \n",
    "    get_session_history = get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"output\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bddcce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:55:48 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "18:55:49 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search` with `{'query': 'Silvia García importancia cooperativa Mediterranea'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'Silvia García importancia cooperativa Mediterranea', 'follow_up_questions': None, 'answer': 'Silvia García is an art director at LA Mediterranea LTD. She has over 25 years of experience in human resources management. She is also recognized for her leadership in cooperative sustainability initiatives.', 'images': [], 'results': [{'url': 'https://es.linkedin.com/in/silvia-garcia-005a2910a', 'title': 'Silvia Garcia - Art director en LA MEDITERRANEA LTD. - LinkedIn', 'content': 'Silvia Garcia\\nArt director en LA MEDITERRANEA LTD.\\nValencian Community, Spain\\n89 connections, 91 followers\\n\\n\\nAbout\\nN/A\\n\\n\\nExperience\\nArt director\\n[LA MEDITERRANEA LTD.](https://uk.linkedin.com/company/la-mediterranea-ltd-)  \\nJan 1986 - Present\\nN/A\\n\\n\\n\\nEducation\\nN/A', 'score': 0.5524679, 'raw_content': None}, {'url': 'https://www.instagram.com/p/DHJa6byxpJ1/', 'title': 'DIRECTIVAS Silvia García es experta en gestión del talento y ...', 'content': 'Silvia García es experta en gestión del talento y desarrollo organizacional, con más de 25 años de experiencia en dirección de recursos humanos', 'score': 0.3658876, 'raw_content': None}, {'url': 'https://co.linkedin.com/in/silvia-garcia-tellez/en', 'title': 'Silvia Garcia - Desarrollo Económico | Impacto Social | Sostenibilidad', 'content': 'Responsible for the design and implementation of a program that aims to strengthen the business capabilities among indigenous peoples and identify and replace', 'score': 0.33509022, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=oY3rR0wVar4', 'title': 'Mensaje de Silvia García de Alba, galardonada con la Presea Pedro ...', 'content': 'La galardonada, Silvia García de Alba refirió que el recibir esta distinción la compromete a seguir adelante a favor de los hidalguenses.', 'score': 0.26564035, 'raw_content': None}, {'url': 'https://www.agroalimentaries.es/wp-content/uploads/2023/07/N58-Julio-2023.pdf', 'title': '[PDF] Cooperativas Agro-alimentarias de España', 'content': 'Silvia Gómez, directora general de la cooperativa onubense sostiene que “la sostenibilidad es un factor clave en el devenir de nuestra cooperativa. Por ello', 'score': 0.2219239, 'raw_content': None}], 'response_time': 1.74, 'request_id': '451650bd-840d-40cb-a1e4-0a03eb1e3e50'}\u001b[0m18:55:53 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32;1m\u001b[1;3mSilvia García es directora de arte en LA MEDITERRANEA LTD. y tiene más de 25 años de experiencia en gestión de recursos humanos. También es reconocida por su liderazgo en iniciativas de sostenibilidad cooperativa. Sin embargo, no se encontró información específica sobre su importancia en una cooperativa llamada Mediterranea. Parece que su rol está más relacionado con la dirección artística y la gestión del talento en la empresa mencionada.\u001b[0m\n",
      "18:55:54 langchain_core.callbacks.manager WARNING   Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'RedisSummaryMemory' object has no attribute 'add_messages'\")\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '¿Silvia García fue importante en la cooperativa Mediterranea?', 'chat_history': [HumanMessage(content='¿Silvia García fue importante en la cooperativa Mediterranea?', additional_kwargs={}, response_metadata={}), AIMessage(content='Silvia García es directora de arte en LA MEDITERRANEA LTD., una empresa ubicada en Valencia, España, y ha estado con la compañía desde 1986. Sin embargo, no se encontró información específica sobre su importancia en una cooperativa llamada Mediterranea. Parece que su rol está más relacionado con el diseño y la dirección artística en la empresa mencionada.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuentame un poco acerca de esta empresa', additional_kwargs={}, response_metadata={}), AIMessage(content='LA MEDITERRANEA LTD. es una empresa ubicada en Valencia, España, que se especializa en la venta de productos alimenticios y bebidas. Fue establecida en 1975 y es conocida por su vajilla de vidrio artesanal. La compañía combina técnicas de fabricación computarizadas con el soplado de vidrio artesanal, y opera en varios mercados, incluyendo el Reino Unido.', additional_kwargs={}, response_metadata={})], 'output': 'Silvia García es directora de arte en LA MEDITERRANEA LTD. y tiene más de 25 años de experiencia en gestión de recursos humanos. También es reconocida por su liderazgo en iniciativas de sostenibilidad cooperativa. Sin embargo, no se encontró información específica sobre su importancia en una cooperativa llamada Mediterranea. Parece que su rol está más relacionado con la dirección artística y la gestión del talento en la empresa mencionada.'}\n"
     ]
    }
   ],
   "source": [
    "# # Pregunta que mezcla web + grafo\n",
    "# q1 = \"¿Silvia García fue importante en la cooperativa Mediterranea?\"\n",
    "\n",
    "# print(agent_with_memory.invoke(\n",
    "#     {\"input\": q1},\n",
    "#     {\"configurable\": {\"session_id\": \"usuario-123\"}}\n",
    "\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# q1 = \"Durante que años estuvo activa?\"\n",
    "# async for ev in agent_with_memory.astream_events(\n",
    "#     {\"input\": q3},\n",
    "#     config={\"configurable\": {\"session_id\": \"prueba_futbol\"}}, \n",
    "#     version=\"v1\"):\n",
    "#     et = ev[\"event\"]\n",
    "\n",
    "#     # ---- Texto del modelo (token a token) ----\n",
    "#     if et == \"on_chat_model_stream\":\n",
    "#         delta = ev[\"data\"][\"chunk\"].content\n",
    "#         print(delta, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:06:44 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "---PROMPT ACTUAL---\n",
      "Quien es el mejor futbolista de la historia?\n",
      "---CHAT HISTORY---\n",
      "[]\n",
      "11:06:45 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32;1m\u001b[1;3mLa pregunta sobre quién es el mejor futbolista de la historia es subjetiva y depende de las opiniones personales de los aficionados al fútbol. Sin embargo, algunos de los nombres más mencionados en este debate incluyen a:\n",
      "\n",
      "1. **Pelé**: Considerado uno de los más grandes de todos los tiempos, Pelé ganó tres Copas del Mundo con Brasil (1958, 1962, 1970) y es conocido por su habilidad, técnica y capacidad goleadora.\n",
      "\n",
      "2. **Diego Maradona**: Famoso por su actuación en la Copa del Mundo de 1986, donde llevó a Argentina al título, Maradona es recordado por su increíble talento y momentos icónicos en el campo.\n",
      "\n",
      "3. **Lionel Messi**: Con múltiples Balones de Oro y una carrera llena de récords y títulos con el FC Barcelona y la selección argentina, Messi es considerado por muchos como el mejor futbolista de la era moderna.\n",
      "\n",
      "4. **Cristiano Ronaldo**: Con una impresionante cantidad de goles y títulos en clubes como el Manchester United, Real Madrid y Juventus, Ronaldo es otro de los grandes de la era contemporánea.\n",
      "\n",
      "5. **Johan Cruyff**: Un innovador del juego, Cruyff dejó una marca indeleble tanto como jugador como entrenador, especialmente con su influencia en el \"fútbol total\" del Ajax y el FC Barcelona.\n",
      "\n",
      "Cada uno de estos jugadores ha dejado un legado único en el fútbol, y la elección del \"mejor\" depende de criterios personales como el estilo de juego, logros y contribuciones al deporte.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "q1 = \"Quien es el mejor futbolista de la historia?\"\n",
    "chat_model_response_printed = False\n",
    "\n",
    "async for ev in agent_with_memory.astream_events(\n",
    "    {\"input\": q1},\n",
    "    config={\"configurable\": {\"session_id\": \"prueba_futbol\"}}, \n",
    "    version=\"v1\"\n",
    "):\n",
    "    if ev[\"event\"] == \"on_prompt_start\":\n",
    "        print(\"---PROMPT ACTUAL---\")\n",
    "        print(ev[\"data\"][\"input\"][\"input\"])\n",
    "        \n",
    "        print(\"---CHAT HISTORY---\")\n",
    "        print(ev[\"data\"][\"input\"][\"chat_history\"])\n",
    "    \n",
    "    if ev[\"event\"] == \"on_chat_model_stream\":\n",
    "        if not chat_model_response_printed:\n",
    "            print(\"---CHAT MODEL RESPONSE---\")\n",
    "            chat_model_response_printed = True\n",
    "        \n",
    "        delta = ev[\"data\"][\"chunk\"].content\n",
    "        print(delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "732e055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:12:43 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "---PROMPT ACTUAL---\n",
      "Cuantos balones de oro ha ganado?\n",
      "---CHAT HISTORY---\n",
      "[HumanMessage(content='Quien es el mejor futbolista de la historia?', additional_kwargs={}, response_metadata={}), AIMessage(content='La pregunta sobre quién es el mejor futbolista de la historia es subjetiva y depende de las opiniones personales de los aficionados al fútbol. Sin embargo, algunos de los nombres más mencionados en este debate incluyen a:\\n\\n1. **Pelé**: Considerado uno de los más grandes de todos los tiempos, Pelé ganó tres Copas del Mundo con Brasil (1958, 1962, 1970) y es conocido por su habilidad, técnica y capacidad goleadora.\\n\\n2. **Diego Maradona**: Famoso por su actuación en la Copa del Mundo de 1986, donde llevó a Argentina al título, Maradona es recordado por su increíble talento y momentos icónicos en el campo.\\n\\n3. **Lionel Messi**: Con múltiples Balones de Oro y una carrera llena de récords y títulos con el FC Barcelona y la selección argentina, Messi es considerado por muchos como el mejor futbolista de la era moderna.\\n\\n4. **Cristiano Ronaldo**: Con una impresionante cantidad de goles y títulos en clubes como el Manchester United, Real Madrid y Juventus, Ronaldo es otro de los grandes de la era contemporánea.\\n\\n5. **Johan Cruyff**: Un innovador del juego, Cruyff dejó una marca indeleble tanto como jugador como entrenador, especialmente con su influencia en el \"fútbol total\" del Ajax y el FC Barcelona.\\n\\nCada uno de estos jugadores ha dejado un legado único en el fútbol, y la elección del \"mejor\" depende de criterios personales como el estilo de juego, logros y contribuciones al deporte.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}), AIMessage(content='Para responder a tu pregunta sobre cuántos Balones de Oro ha ganado un futbolista específico, necesitaría saber a quién te refieres. Los más destacados en este premio son Lionel Messi y Cristiano Ronaldo:\\n\\n- **Lionel Messi** ha ganado el Balón de Oro 7 veces (hasta 2021).\\n- **Cristiano Ronaldo** ha ganado el Balón de Oro 5 veces (hasta 2021).\\n\\nSi te refieres a otro jugador, por favor indícalo para poder proporcionarte la información correcta.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}), AIMessage(content='Parece que tu pregunta no especifica a qué futbolista te refieres. Si estás preguntando por Lionel Messi, él ha ganado el Balón de Oro 7 veces hasta 2021. Si te refieres a Cristiano Ronaldo, él ha ganado el Balón de Oro 5 veces hasta 2021. Si tienes en mente a otro jugador, por favor indícalo para que pueda darte la información correcta.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}), AIMessage(content='Lionel Messi ha ganado el Balón de Oro 8 veces, con su más reciente galardón en 2023.', additional_kwargs={}, response_metadata={})]\n",
      "11:12:44 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "---CHAT MODEL RESPONSE---\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search` with `{'query': 'cuántos Balones de Oro ha ganado Lionel Messi', 'search_depth': 'basic'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'cuántos Balones de Oro ha ganado Lionel Messi', 'follow_up_questions': None, 'answer': 'Lionel Messi ha ganado el Balón de Oro ocho veces. Las ediciones fueron en 2009, 2010, 2011, 2012, 2015, 2019, 2021 y 2023.', 'images': [], 'results': [{'url': 'https://www.si.com/es-us/futbol/cuantos-balones-de-oro-ha-ganado-lionel-messi-a-lo-largo-de-su-carrera', 'title': '¿Cuántos Balones de Oro ha ganado Lionel Messi a lo ...', 'content': 'Messi es el que más ha ganado el Balón de Oro, triunfando en ocho oportunidades. El primer éxito del \"10\" llegó en 2009, tras marcar en la final', 'score': 0.8987045, 'raw_content': None}, {'url': 'https://www.marca.com/futbol/balon-oro/2024/10/28/cuantos-balones-oro-leo-messi-cristiano-ronaldo.html', 'title': 'Cuántos balones de oro tienen Leo Messi y Cristiano ...', 'content': 'Lionel Messi, el talentoso argentino, ha sido galardonado con el Balón de Oro en nueve ocasiones, consolidándose como el futbolista más premiado', 'score': 0.8909888, 'raw_content': None}, {'url': 'https://www.mundodeportivo.com/uncomo/deporte/articulo/cuantos-balones-de-oro-tiene-messi-descubre-cuantos-gano-55173.html', 'title': '¿Cuántos balones de oro tiene Messi? Descubre ...', 'content': 'Leo Messi lidera el ránking de balones de oro con un total de 8. El primero lo consiguió en diciembre de 2009, superando en las votaciones a', 'score': 0.88372874, 'raw_content': None}, {'url': 'https://www.tiktok.com/@adictosfutbol/video/7362641295195557162?lang=es', 'title': 'Messi a los 25 años ya tenia 4 Balones de Oro ...', 'content': 'No vas a creer cómo messi, a la edad de 25 años ya había ganado 4 balones de oro. Hoy en día no hay un solo futbolista menor a 25 años. que', 'score': 0.85797083, 'raw_content': None}, {'url': 'https://www.dazn.com/es-MX/news/f%C3%BAtbol/cuantos-balones-oro-leo-messi/lqeidd5kuq1111h3n3zxixkyu', 'title': '¿Cuántos balones de oro tiene Leo Messi?', 'content': '¿Cuántos balones de oro tiene Leo Messi? · Lionel Messi (8): 2009, 2010, 2011, 2012, 2015, 2019, 2021 y 2023. · Cristiano Ronaldo (5): 2008, 2013,', 'score': 0.8464638, 'raw_content': None}], 'response_time': 1.83, 'request_id': 'ec020ed6-6a5f-4e7c-bab3-ad2435603f9e'}\u001b[0m---PROMPT ACTUAL---\n",
      "Cuantos balones de oro ha ganado?\n",
      "---CHAT HISTORY---\n",
      "[HumanMessage(content='Quien es el mejor futbolista de la historia?', additional_kwargs={}, response_metadata={}), AIMessage(content='La pregunta sobre quién es el mejor futbolista de la historia es subjetiva y depende de las opiniones personales de los aficionados al fútbol. Sin embargo, algunos de los nombres más mencionados en este debate incluyen a:\\n\\n1. **Pelé**: Considerado uno de los más grandes de todos los tiempos, Pelé ganó tres Copas del Mundo con Brasil (1958, 1962, 1970) y es conocido por su habilidad, técnica y capacidad goleadora.\\n\\n2. **Diego Maradona**: Famoso por su actuación en la Copa del Mundo de 1986, donde llevó a Argentina al título, Maradona es recordado por su increíble talento y momentos icónicos en el campo.\\n\\n3. **Lionel Messi**: Con múltiples Balones de Oro y una carrera llena de récords y títulos con el FC Barcelona y la selección argentina, Messi es considerado por muchos como el mejor futbolista de la era moderna.\\n\\n4. **Cristiano Ronaldo**: Con una impresionante cantidad de goles y títulos en clubes como el Manchester United, Real Madrid y Juventus, Ronaldo es otro de los grandes de la era contemporánea.\\n\\n5. **Johan Cruyff**: Un innovador del juego, Cruyff dejó una marca indeleble tanto como jugador como entrenador, especialmente con su influencia en el \"fútbol total\" del Ajax y el FC Barcelona.\\n\\nCada uno de estos jugadores ha dejado un legado único en el fútbol, y la elección del \"mejor\" depende de criterios personales como el estilo de juego, logros y contribuciones al deporte.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}), AIMessage(content='Para responder a tu pregunta sobre cuántos Balones de Oro ha ganado un futbolista específico, necesitaría saber a quién te refieres. Los más destacados en este premio son Lionel Messi y Cristiano Ronaldo:\\n\\n- **Lionel Messi** ha ganado el Balón de Oro 7 veces (hasta 2021).\\n- **Cristiano Ronaldo** ha ganado el Balón de Oro 5 veces (hasta 2021).\\n\\nSi te refieres a otro jugador, por favor indícalo para poder proporcionarte la información correcta.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}), AIMessage(content='Parece que tu pregunta no especifica a qué futbolista te refieres. Si estás preguntando por Lionel Messi, él ha ganado el Balón de Oro 7 veces hasta 2021. Si te refieres a Cristiano Ronaldo, él ha ganado el Balón de Oro 5 veces hasta 2021. Si tienes en mente a otro jugador, por favor indícalo para que pueda darte la información correcta.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}), AIMessage(content='Lionel Messi ha ganado el Balón de Oro 8 veces, con su más reciente galardón en 2023.', additional_kwargs={}, response_metadata={})]\n",
      "11:12:47 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Lionel Messi ha ganado el Balón de Oro ocho veces, en los años 2009, 2010, 2011, 2012, 2015, 2019, 2021 y 2023.\u001b[32;1m\u001b[1;3mLionel Messi ha ganado el Balón de Oro ocho veces, en los años 2009, 2010, 2011, 2012, 2015, 2019, 2021 y 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "q2 = \"Cuantos balones de oro ha ganado?\"\n",
    "chat_model_response_printed = False\n",
    "\n",
    "async for ev in agent_with_memory.astream_events(\n",
    "    {\"input\": q2},\n",
    "    config={\"configurable\": {\"session_id\": \"prueba_futbol\"}}, \n",
    "    version=\"v1\"\n",
    "):\n",
    "    if ev[\"event\"] == \"on_prompt_start\":\n",
    "        print(\"---PROMPT ACTUAL---\")\n",
    "        print(ev[\"data\"][\"input\"][\"input\"])\n",
    "        \n",
    "        print(\"---CHAT HISTORY---\")\n",
    "        print(ev[\"data\"][\"input\"][\"chat_history\"])\n",
    "    \n",
    "    if ev[\"event\"] == \"on_chat_model_stream\":\n",
    "        if not chat_model_response_printed:\n",
    "            print(\"---CHAT MODEL RESPONSE---\")\n",
    "            chat_model_response_printed = True\n",
    "        \n",
    "        delta = ev[\"data\"][\"chunk\"].content\n",
    "        print(delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "345e2ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:53:09 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "history = get_session_history(\"prueba_futbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bca0d4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Quien es el mejor futbolista de la historia?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='La pregunta sobre quién es el mejor futbolista de la historia es subjetiva y depende de las opiniones personales de los aficionados al fútbol. Sin embargo, algunos de los nombres más mencionados en este debate incluyen a:\\n\\n1. **Pelé**: Considerado uno de los más grandes de todos los tiempos, Pelé ganó tres Copas del Mundo con Brasil (1958, 1962, 1970) y es conocido por su habilidad, técnica y capacidad goleadora.\\n\\n2. **Diego Maradona**: Famoso por su actuación en la Copa del Mundo de 1986, donde llevó a Argentina al título, Maradona es recordado por su increíble talento y momentos icónicos en el campo.\\n\\n3. **Lionel Messi**: Con múltiples Balones de Oro y una carrera llena de récords y títulos con el FC Barcelona y la selección argentina, Messi es considerado por muchos como el mejor futbolista de la era moderna.\\n\\n4. **Cristiano Ronaldo**: Con una impresionante cantidad de goles y títulos en clubes como el Manchester United, Real Madrid y Juventus, Ronaldo es otro de los grandes de la era contemporánea.\\n\\n5. **Johan Cruyff**: Un innovador del juego, Cruyff dejó una marca indeleble tanto como jugador como entrenador, especialmente con su influencia en el \"fútbol total\" del Ajax y el FC Barcelona.\\n\\nCada uno de estos jugadores ha dejado un legado único en el fútbol, y la elección del \"mejor\" depende de criterios personales como el estilo de juego, logros y contribuciones al deporte.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Para responder a tu pregunta sobre cuántos Balones de Oro ha ganado un futbolista específico, necesitaría saber a quién te refieres. Los más destacados en este premio son Lionel Messi y Cristiano Ronaldo:\\n\\n- **Lionel Messi** ha ganado el Balón de Oro 7 veces (hasta 2021).\\n- **Cristiano Ronaldo** ha ganado el Balón de Oro 5 veces (hasta 2021).\\n\\nSi te refieres a otro jugador, por favor indícalo para poder proporcionarte la información correcta.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Parece que tu pregunta no especifica a qué futbolista te refieres. Si estás preguntando por Lionel Messi, él ha ganado el Balón de Oro 7 veces hasta 2021. Si te refieres a Cristiano Ronaldo, él ha ganado el Balón de Oro 5 veces hasta 2021. Si tienes en mente a otro jugador, por favor indícalo para que pueda darte la información correcta.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Lionel Messi ha ganado el Balón de Oro 8 veces, con su más reciente galardón en 2023.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Cuantos balones de oro ha ganado?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Lionel Messi ha ganado el Balón de Oro ocho veces, en los años 2009, 2010, 2011, 2012, 2015, 2019, 2021 y 2023.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bbe34ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/l10s0t8n3cd4ghd9mjhdlltw0000gn/T/ipykernel_13289/4133568506.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm_summarizer = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_summarizer = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Resume la conversación de forma objetiva y comprimida, en español. \"\n",
    "     \"Conserva: hechos, decisiones, tareas pendientes, fuentes citadas, parámetros importantes. \"\n",
    "     \"No repitas texto literal salvo nombres/IDs. Limita a ~20 palabras.\"),\n",
    "    MessagesPlaceholder(\"conversation\"),\n",
    "    (\"human\", \"Crea un resumen útil para continuar la charla sin perder contexto.\")\n",
    "])\n",
    "summary_chain = summary_prompt | llm_summarizer | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "05b3e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, BaseMessage\n",
    "\n",
    "def summarize_into_system_message(messages: List[BaseMessage]) -> SystemMessage:\n",
    "    summary_text = summary_chain.invoke({\"conversation\": messages})\n",
    "    return SystemMessage(content=f\"Resumen hasta ahora: {summary_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ee3c6ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:56:54 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "msgs = history.messages\n",
    "summary_msg = summarize_into_system_message(msgs)\n",
    "history.clear()\n",
    "history.add_message(summary_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "37fcf626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Resumen hasta ahora: La conversación se centra en la cantidad de Balones de Oro ganados por Lionel Messi, quien ha obtenido el premio ocho veces, incluyendo en 2023.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YT_GRAPH_RAG_VENV (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
